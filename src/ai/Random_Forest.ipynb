{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dyr5S20Y2QR"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor # RandomForestRegressor 모델 (Random Forest Regressor model)\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV # 데이터 분할 및 하이퍼파라미터 튜닝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tasyn6C1clKj",
        "outputId": "c76cca09-c8aa-4930-94d8-198cf741dca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "total 11\n",
            "-rw------- 1 root root 3437 Oct 15 10:00 data_load.py\n",
            "-rw------- 1 root root 3026 Oct 15 11:08 preprocessor.py\n",
            "drwx------ 2 root root 4096 Oct 14 14:11 __pycache__\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls -l /content/drive/MyDrive/utils/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69s2VFyrPiND",
        "outputId": "e47014e9-132c-4541-9cfa-d8662a4dac89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive 마운트 성공! (Google Drive mounted successfully!)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✅ Google Drive 마운트 성공! (Google Drive mounted successfully!)\")\n",
        "except Exception as e:\n",
        "    if \"already mounted\" in str(e).lower():\n",
        "        print(\"ℹ️ Google Drive는 이미 마운트되어 있습니다. (Google Drive is already mounted.)\")\n",
        "    else:\n",
        "        print(f\"❌ Google Drive 마운트 중 오류 발생 (Error mounting Google Drive): {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UzHsPBbPmiQ",
        "outputId": "bfbd3c32-d583-4b25-a84c-fa8945f1d758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ '/content/drive/MyDrive/utils' 경로 추가 완료 (Path added successfully)\n"
          ]
        }
      ],
      "source": [
        "utils_path = '/content/drive/MyDrive/utils'\n",
        "\n",
        "# sys.path에 utils 경로 추가 (Add utils path to sys.path)\n",
        "if utils_path not in sys.path:\n",
        "    sys.path.append(utils_path)\n",
        "    print(f\"✅ '{utils_path}' 경로 추가 완료 (Path added successfully)\")\n",
        "else:\n",
        "    print(f\"ℹ️ '{utils_path}' 경로는 이미 sys.path에 존재합니다. (Path already exists in sys.path.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpxiq-tcPy9I",
        "outputId": "6ac71058-d7c8-4479-dcb3-b763c88901d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ data_load 모듈 임포트 성공 (data_load module imported successfully)\n",
            "🔍 ['2025-01-01'부터 '2025-09-14'까지의 데이터를 로드]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/utils/data_load.py:48: DtypeWarning: Columns (19,20,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  temp_df = pd.read_csv(\n",
            "/content/drive/MyDrive/utils/data_load.py:66: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_df = pd.concat(dfs_to_concat, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 총 257개 파일, 8474140개 행의 데이터를 성공적으로 합쳤습니다.\n"
          ]
        }
      ],
      "source": [
        "# 데이터 로드 모듈 임포트 및 실행 (Import and run data load module)\n",
        "try:\n",
        "    import data_load\n",
        "    print(\"✅ data_load 모듈 임포트 성공 (data_load module imported successfully)\")\n",
        "    # 데이터 로드 (Load data - start/end dates are examples, modify as needed)\n",
        "    combined_df = data_load.load_data(start_date='2025-01-01', end_date='2025-09-14')\n",
        "    if combined_df is None or combined_df.empty:\n",
        "        print(\"❌ 데이터 로드 실패 또는 데이터프레임이 비어있습니다. 스크립트를 중단합니다. (Data load failed or DataFrame is empty. Stopping script.)\")\n",
        "        # exit()\n",
        "except ImportError as e:\n",
        "    print(f\"❌ data_load 모듈 임포트 중 오류 발생 (Error importing data_load module): {e}\")\n",
        "    print(f\"'{utils_path}' 경로에 data_load.py 파일이 있는지 확인하세요. (Check if data_load.py exists in '{utils_path}'.)\")\n",
        "    combined_df = None # 오류 시 None으로 초기화 (Initialize to None on error)\n",
        "    # exit()\n",
        "except Exception as e:\n",
        "    print(f\"❌ 데이터 로드 중 예상치 못한 오류 발생 (Unexpected error during data load): {e}\")\n",
        "    combined_df = None\n",
        "    # exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHB6VCbXQW-T",
        "outputId": "71405c8b-1a59-424b-f233-e71c99bda1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ preprocessor 모듈 임포트 성공 (preprocessor module imported successfully)\n",
            "\n",
            "--- 데이터 전처리 시작 (Starting data preprocessing) ---\n",
            "ℹ️ 다음 object 컬럼을 문자열로 변환합니다 (Converting the following object columns to string): ['생성일', '급수펌프 입력', '배기가스온도2', '배기가스온도3', '배기 재 순환 온도', '에코 온도1', '에코 온도2', '버너온도', '재순환 O2', '재순환 NOx', '운전시간', '확률 업데이트 시간']\n",
            "✅ Object 타입 컬럼 문자열 변환 완료 (Object type column conversion to string complete - some may be skipped on error)\n",
            "preprocessor.preprocessor 함수 실행 중...\n",
            "--- 데이터 전처리 시작 ---\n",
            "✅ 1. 불필요한 컬럼 14개 제거 완료\n",
            "✅ 2-1. 범주형 데이터 결측치 처리 완료\n",
            "✅ 2-2. 범주형 변수 변환 완료\n",
            "ℹ️ 2-3. 처리할 수치형 결측치가 없습니다.\n",
            "✅ 3. 피처 스케일링 완료\n",
            "--- 데이터 전처리 완료 ---\n",
            "✅ 전처리 함수 실행 완료. 형상: (8474140, 33) (Preprocessing function execution complete. Shape: (8474140, 33))\n",
            "✅ '효율(순간)' 100 이상인 행 0개 제거 완료 (Removed 0 rows where '효율(순간)' >= 100)\n",
            "최종 전처리 데이터 형상: (8474140, 33) (Final preprocessed data shape: (8474140, 33))\n",
            "--- 데이터 전처리 완료 (Data preprocessing complete) ---\n"
          ]
        }
      ],
      "source": [
        "# ## 데이터 전처리 (Data Preprocessing)\n",
        "\n",
        "# preprocessor.py 모듈을 임포트하고, 로드된 데이터에 전처리 함수를 적용합니다.\n",
        "# Import the preprocessor.py module and apply the preprocessing function to the loaded data.\n",
        "preprocessor = None # preprocessor 변수 초기화\n",
        "expected_preprocessor_path = os.path.join(utils_path, 'preprocessor.py') # 예상 경로 미리 정의\n",
        "try:\n",
        "    import preprocessor\n",
        "    print(\"✅ preprocessor 모듈 임포트 성공 (preprocessor module imported successfully)\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ preprocessor 모듈 임포트 중 오류 발생 (Error importing preprocessor module): {e}\")\n",
        "    print(f\"'{utils_path}' 경로에 preprocessor.py 파일이 있는지 확인하세요. (Check if preprocessor.py exists in '{utils_path}'.)\")\n",
        "    preprocessor = None # 오류 시 None으로 초기화 (Initialize to None on error)\n",
        "    # exit()\n",
        "\n",
        "\n",
        "# 데이터 전처리 실행 (Run data preprocessing)\n",
        "preprocessed_df = None # 전처리 후 데이터프레임 변수 초기화 (Initialize preprocessed DataFrame variable)\n",
        "if 'combined_df' in locals() and combined_df is not None and 'preprocessor' in locals() and preprocessor is not None:\n",
        "    print(\"\\n--- 데이터 전처리 시작 (Starting data preprocessing) ---\")\n",
        "\n",
        "    # Object 타입 컬럼을 문자열로 변환 (Convert object type columns to string)\n",
        "    # This step is crucial if the preprocessor uses tools like LabelEncoder that expect uniform data types.\n",
        "    object_cols = combined_df.select_dtypes(include=['object']).columns\n",
        "    if not object_cols.empty:\n",
        "        print(f\"ℹ️ 다음 object 컬럼을 문자열로 변환합니다 (Converting the following object columns to string): {list(object_cols)}\")\n",
        "        for col in object_cols:\n",
        "            try:\n",
        "                # Missing values might cause issues during conversion; consider filling them first if necessary.\n",
        "                # Example: combined_df[col].fillna('Unknown', inplace=True)\n",
        "                combined_df[col] = combined_df[col].astype(str)\n",
        "            except Exception as e:\n",
        "                 print(f\"⚠️ 컬럼 '{col}' 변환 중 오류 (Error converting column '{col}'): {e}. 해당 컬럼 처리를 건너뛸 수 있습니다. (May skip processing this column.)\")\n",
        "        print(\"✅ Object 타입 컬럼 문자열 변환 완료 (Object type column conversion to string complete - some may be skipped on error)\")\n",
        "    else:\n",
        "        print(\"ℹ️ 문자열로 변환할 Object 타입 컬럼이 없습니다. (No object type columns to convert to string.)\")\n",
        "\n",
        "    # 전처리 함수 호출 (Call preprocessing function)\n",
        "    try:\n",
        "        # Using copy() is recommended to preserve the original combined_df.\n",
        "        print(\"preprocessor.preprocessor 함수 실행 중...\")\n",
        "        preprocessed_df = preprocessor.preprocessor(combined_df.copy())\n",
        "        print(f\"✅ 전처리 함수 실행 완료. 형상: {preprocessed_df.shape} (Preprocessing function execution complete. Shape: {preprocessed_df.shape})\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 전처리 함수 실행 중 오류 발생 (Error executing preprocessing function): {e}\")\n",
        "        # exit() # 오류 발생 시 중단 (Exit on error)\n",
        "\n",
        "    # '효율(순간)' 값이 100 이상인 비정상 데이터 제거 (Remove outliers where '효율(순간)' >= 100)\n",
        "    if preprocessed_df is not None:\n",
        "        try:\n",
        "            target_col = '효율(순간)' # 목표 변수 컬럼명 (Target variable column name)\n",
        "            if target_col in preprocessed_df.columns:\n",
        "                initial_rows = len(preprocessed_df)\n",
        "                # Filter out rows where the target value is abnormally high (>= 100)\n",
        "                preprocessed_df = preprocessed_df[preprocessed_df[target_col] < 100].copy()\n",
        "                removed_rows = initial_rows - len(preprocessed_df)\n",
        "                print(f\"✅ '{target_col}' 100 이상인 행 {removed_rows}개 제거 완료 (Removed {removed_rows} rows where '{target_col}' >= 100)\")\n",
        "                print(f\"최종 전처리 데이터 형상: {preprocessed_df.shape} (Final preprocessed data shape: {preprocessed_df.shape})\")\n",
        "                print(\"--- 데이터 전처리 완료 (Data preprocessing complete) ---\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ '{target_col}' 100 이상 행 제거 중 오류 (Error removing rows where '{target_col}' >= 100): {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etGFhDf1YoCY",
        "outputId": "af61380c-b325-4356-d6c8-ceba6610edfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 데이터 학습/테스트 분할 완료 (Data split into train/test complete) (80%/20%)\n",
            "학습 데이터 X 형상 (Train X shape): (6779312, 32), 테스트 데이터 X 형상 (Test X shape): (1694828, 32)\n",
            "학습 데이터 y 형상 (Train y shape): (6779312,), 테스트 데이터 y 형상 (Test y shape): (1694828,)\n"
          ]
        }
      ],
      "source": [
        "# ## 데이터 분할 (Data Splitting)\n",
        "\n",
        "# 전처리된 데이터를 학습용(Train)과 테스트용(Test)으로 분할합니다 (보통 80:20 비율).\n",
        "# Split the preprocessed data into training and testing sets (typically 80:20 ratio).\n",
        "X_train, X_test, y_train, y_test = None, None, None, None # Initialize variables to None\n",
        "# Proceed only if preprocessing was successful\n",
        "if 'preprocessed_df' in locals() and preprocessed_df is not None and not preprocessed_df.empty:\n",
        "    try:\n",
        "        y_col = '효율(순간)' # Define the name of the dependent variable (target) column\n",
        "        # Ensure the target column exists before attempting to drop/select it\n",
        "        if y_col not in preprocessed_df.columns:\n",
        "            raise KeyError(f\"'{y_col}' 컬럼이 전처리된 데이터프레임에 없습니다. (Column '{y_col}' not found in preprocessed DataFrame.)\")\n",
        "\n",
        "        # Separate features (X) and target (y)\n",
        "        X = preprocessed_df.drop(columns=[y_col]) # Independent variables (Features)\n",
        "        y = preprocessed_df[y_col]               # Dependent variable (Target)\n",
        "\n",
        "        # Split data into training and test sets using train_test_split\n",
        "        # test_size=0.2 means 20% for testing, 80% for training\n",
        "        # random_state ensures the split is the same every time the code runs (for reproducibility)\n",
        "        # shuffle=True (default) shuffles data before splitting\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "        # Print shapes of the resulting datasets to confirm the split\n",
        "        print(\"\\n✅ 데이터 학습/테스트 분할 완료 (Data split into train/test complete) (80%/20%)\")\n",
        "        print(f\"학습 데이터 X 형상 (Train X shape): {X_train.shape}, 테스트 데이터 X 형상 (Test X shape): {X_test.shape}\")\n",
        "        print(f\"학습 데이터 y 형상 (Train y shape): {y_train.shape}, 테스트 데이터 y 형상 (Test y shape): {y_test.shape}\")\n",
        "\n",
        "    # Handle error if the target column is missing\n",
        "    except KeyError as e:\n",
        "        print(f\"❌ 데이터 분할 중 오류 발생: {e}\")\n",
        "    # Handle any other unexpected errors during splitting\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 데이터 분할 중 예상치 못한 오류 발생 (Unexpected error during data split): {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWrEEXb3USRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5642042-0a9f-491c-c9c9-f477e3d314ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 하이퍼파라미터 튜닝 시작 (Starting Hyperparameter Tuning - RandomizedSearchCV for RandomForest) ---\n",
            "10개 조합, 2-겹 교차 검증으로 Randomized Search 실행... (Running Randomized Search with 10 combinations and 2-fold CV...)\n",
            "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
          ]
        }
      ],
      "source": [
        "# ## 하이퍼파라미터 튜닝 (Hyperparameter Tuning)\n",
        "\n",
        "# RandomizedSearchCV를 사용하여 Random Forest 모델의 최적 하이퍼파라미터 조합을 탐색합니다.\n",
        "# Use RandomizedSearchCV to find the optimal combination of hyperparameters for the Random Forest model.\n",
        "best_params_rf = None # Initialize variable to store best parameters\n",
        "# Proceed only if training data is available\n",
        "if X_train is not None and y_train is not None:\n",
        "    print(\"\\n--- 하이퍼파라미터 튜닝 시작 (Starting Hyperparameter Tuning - RandomizedSearchCV for RandomForest) ---\")\n",
        "    # Initialize the Random Forest Regressor model for tuning\n",
        "    # random_state for reproducibility, n_jobs=-1 to use all CPU cores\n",
        "    rf_model_tune = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Define the parameter space (distributions) to sample from during Randomized Search\n",
        "    param_dist_rf = {\n",
        "        'n_estimators': np.arange(50, 501, 50),     # Number of trees: 50, 100, ..., 500\n",
        "        'max_depth': [None] + list(np.arange(5, 31, 5)), # Max depth: None (unlimited), 5, 10, ..., 30\n",
        "        'min_samples_split': [2, 5, 10, 15],         # Min samples to split a node: 2, 5, 10, 15\n",
        "        'min_samples_leaf': [1, 3, 5, 7],            # Min samples at a leaf node: 1, 3, 5, 7\n",
        "        'max_features': ['sqrt', 'log2', 0.5, 0.7] # Max features for split: sqrt(n_features), log2(n_features), 50%, 70%\n",
        "        # 'bootstrap': [True, False] # Whether bootstrap samples are used (optional)\n",
        "    }\n",
        "\n",
        "    # Configure RandomizedSearchCV\n",
        "    # n_iter: Number of parameter settings that are sampled. Trades off runtime vs quality of the solution.\n",
        "    # cv: Number of cross-validation folds.\n",
        "    # scoring: Strategy to evaluate the performance of the cross-validated model on the test set. 'neg_mean_squared_error' is common for regression.\n",
        "    # verbose: Controls the verbosity: the higher, the more messages.\n",
        "    # n_jobs=-1: Use all available CPU cores for parallel processing.\n",
        "    n_iterations = 10 # Number of iterations (parameter combinations to try). Adjust based on available time.\n",
        "    cv_folds = 2      # Number of cross-validation folds. Adjust based on available time/data size.\n",
        "\n",
        "    random_search_rf = RandomizedSearchCV(\n",
        "        estimator=rf_model_tune,           # The model to tune\n",
        "        param_distributions=param_dist_rf, # The distributions to sample parameters from\n",
        "        n_iter=n_iterations,               # Number of parameter settings that are sampled\n",
        "        scoring='neg_mean_squared_error',  # Metric to evaluate the performance (negative MSE)\n",
        "        cv=cv_folds,                       # Number of cross-validation folds\n",
        "        verbose=1,                         # Print progress messages\n",
        "        random_state=42,                   # Seed for reproducibility\n",
        "        n_jobs=-1                          # Use all available CPU cores\n",
        "    )\n",
        "\n",
        "    # Randomized Search 실행 (Execute Randomized Search on the training data)\n",
        "    try:\n",
        "        print(f\"{n_iterations}개 조합, {cv_folds}-겹 교차 검증으로 Randomized Search 실행... (Running Randomized Search with {n_iterations} combinations and {cv_folds}-fold CV...)\")\n",
        "        # Fit RandomizedSearchCV to find the best parameters\n",
        "        random_search_rf.fit(X_train, y_train)\n",
        "        # Store the best parameters found\n",
        "        best_params_rf = random_search_rf.best_params_\n",
        "        print(\"\\n✅ 하이퍼파라미터 튜닝 완료 (Hyperparameter tuning complete)\")\n",
        "        print(\"최적 하이퍼파라미터 (Best hyperparameters - Random Forest):\", best_params_rf)\n",
        "        # The best_score_ attribute gives the score for the best parameters found.\n",
        "        # Since scoring='neg_mean_squared_error', a higher score (closer to 0) is better.\n",
        "        print(f\"최고 교차 검증 점수 (Best cross-validation score - Negative MSE): {random_search_rf.best_score_:.6f}\")\n",
        "    # Handle potential errors during the tuning process\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 하이퍼파라미터 튜닝 중 오류 발생 (Error during hyperparameter tuning): {e}\")\n",
        "        best_params_rf = None # Reset best parameters on error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RyfN5ZgD-wo"
      },
      "outputs": [],
      "source": [
        " ## 최종 모델 학습 및 평가 (Final Model Training & Evaluation)\n",
        "\n",
        "# RandomizedSearchCV를 통해 찾은 최적 하이퍼파라미터를 사용하여 최종 Random Forest 모델을 학습시키고, 테스트 데이터로 성능을 평가합니다.\n",
        "# Train the final Random Forest model using the best hyperparameters found by RandomizedSearchCV and evaluate its performance on the test data.\n",
        "rf_final_model = None # Initialize final model variable to None\n",
        "# Proceed only if best parameters were found and training data exists\n",
        "if best_params_rf is not None and X_train is not None and y_train is not None:\n",
        "    print(\"\\n--- 최적 파라미터로 Random Forest 모델 학습 및 평가 시작 (Starting final Random Forest model training and evaluation with best parameters) ---\")\n",
        "    # Initialize the final model using the best parameters found\n",
        "    # The ** operator unpacks the dictionary into keyword arguments\n",
        "    rf_final_model = RandomForestRegressor(\n",
        "        **best_params_rf,\n",
        "        random_state=42, # Ensure reproducibility for the final model\n",
        "        n_jobs=-1        # Use all CPU cores\n",
        "    )\n",
        "\n",
        "    # 모델 학습 (Train the final model on the entire training dataset)\n",
        "    try:\n",
        "        print(\"최적 모델 학습 중... (Training the optimal model...)\")\n",
        "        # Fit the model\n",
        "        rf_final_model.fit(X_train, y_train)\n",
        "        print(\"✅ 모델 학습 완료 (Model training complete)\")\n",
        "\n",
        "        # 예측 수행 (Make predictions on both training and test data)\n",
        "        print(\"예측 수행 중... (Making predictions...)\")\n",
        "        y_train_pred_rf = rf_final_model.predict(X_train) # Predictions on training data\n",
        "        y_test_pred_rf = rf_final_model.predict(X_test)   # Predictions on test data\n",
        "        print(\"✅ 예측 수행 완료 (Prediction complete)\")\n",
        "\n",
        "        # 평가 지표 계산 (Calculate various evaluation metrics)\n",
        "        print(\"평가 지표 계산 중... (Calculating evaluation metrics...)\")\n",
        "        # R-squared (Coefficient of Determination)\n",
        "        train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
        "        test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
        "        # Mean Squared Error (MSE)\n",
        "        train_mse_rf = mean_squared_error(y_train, y_train_pred_rf)\n",
        "        test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
        "        # Mean Absolute Error (MAE)\n",
        "        train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
        "        test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
        "        # Root Mean Squared Error (RMSE)\n",
        "        train_rmse_rf = np.sqrt(train_mse_rf)\n",
        "        test_rmse_rf = np.sqrt(test_mse_rf)\n",
        "\n",
        "        # MAPE 계산 함수 정의 (Define MAPE function to handle potential division by zero)\n",
        "        def mean_absolute_percentage_error(y_true, y_pred):\n",
        "            # Convert inputs to numpy arrays\n",
        "            y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "            # Create a mask for non-zero true values to avoid division by zero\n",
        "            non_zero_mask = y_true != 0\n",
        "            # If all true values are zero, return NaN\n",
        "            if np.sum(non_zero_mask) == 0: return np.nan\n",
        "            # Calculate MAPE only for non-zero true values and take the mean\n",
        "            return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
        "\n",
        "        # Calculate Mean Absolute Percentage Error (MAPE)\n",
        "        train_mape_rf = mean_absolute_percentage_error(y_train, y_train_pred_rf)\n",
        "        test_mape_rf = mean_absolute_percentage_error(y_test, y_test_pred_rf)\n",
        "\n",
        "        # 결과 출력 (Print the calculated evaluation metrics)\n",
        "        print(\"\\n--- Random Forest 모델 평가 결과 (튜닝 후) (Random Forest Model Evaluation Results - Tuned) ---\")\n",
        "        print(f\"Train R2: {train_r2_rf:.6f}\")\n",
        "        print(f\"Test R2 : {test_r2_rf:.6f}\")\n",
        "        print(f\"Train MSE: {train_mse_rf:.6f}\")\n",
        "        print(f\"Test MSE : {test_mse_rf:.6f}\")\n",
        "        print(f\"Train MAE: {train_mae_rf:.6f}\")\n",
        "        print(f\"Test MAE : {test_mae_rf:.6f}\")\n",
        "        print(f\"Train RMSE: {train_rmse_rf:.6f}\")\n",
        "        print(f\"Test RMSE : {test_rmse_rf:.6f}\")\n",
        "        # Check if MAPE is NaN before formatting\n",
        "        train_mape_str = f\"{train_mape_rf:.4f}%\" if not np.isnan(train_mape_rf) else \"N/A (y_train contains zeros)\"\n",
        "        test_mape_str = f\"{test_mape_rf:.4f}%\" if not np.isnan(test_mape_rf) else \"N/A (y_test contains zeros)\"\n",
        "        print(f\"Train MAPE: {train_mape_str}\")\n",
        "        print(f\"Test MAPE : {test_mape_str}\")\n",
        "\n",
        "    # Handle potential errors during final model training or evaluation\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 모델 학습 또는 평가 중 오류 발생 (Error during model training or evaluation): {e}\")\n",
        "        rf_final_model = None # Reset final model variable on error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMGpL-s2ECSU"
      },
      "outputs": [],
      "source": [
        "# ## 특성 중요도 분석 (Feature Importance Analysis)\n",
        "\n",
        "# 학습된 최종 Random Forest 모델의 특성 중요도를 확인하고 시각화합니다.\n",
        "# Check and visualize the feature importances of the trained final Random Forest model.\n",
        "# Proceed only if the final model was trained successfully and training data exists\n",
        "if rf_final_model is not None and X_train is not None:\n",
        "    print(\"\\n--- Random Forest 특성 중요도 확인 (Checking Random Forest Feature Importances) ---\")\n",
        "    try:\n",
        "        # Extract feature importances from the trained model\n",
        "        importances_rf = rf_final_model.feature_importances_\n",
        "        # Get the names of the features\n",
        "        feature_names_rf = X_train.columns\n",
        "        # Create a DataFrame for easier handling and sorting\n",
        "        feature_importance_df_rf = pd.DataFrame({'feature': feature_names_rf, 'importance': importances_rf})\n",
        "        # Sort features by importance in descending order\n",
        "        feature_importance_df_rf = feature_importance_df_rf.sort_values(by='importance', ascending=False)\n",
        "\n",
        "        # Print the top 10 most important features\n",
        "        print(\"상위 10개 특성 중요도 (Top 10 Feature Importances - Random Forest):\")\n",
        "        print(feature_importance_df_rf.head(10))\n",
        "\n",
        "        # 특성 중요도 시각화 (Visualize feature importances - Top 10)\n",
        "        plt.figure(figsize=(12, 7)) # Adjust figure size for better readability\n",
        "        top_n = 10\n",
        "        # Select top N features and sort them ascending for plotting (makes highest importance appear at the top)\n",
        "        top_features_rf = feature_importance_df_rf.head(top_n).sort_values(by='importance', ascending=True)\n",
        "\n",
        "        # Create horizontal bar plot using seaborn for potentially better aesthetics\n",
        "        sns.barplot(x='importance', y='feature', data=top_features_rf, palette='viridis') # Use a color palette\n",
        "        # plt.barh(top_features_rf['feature'], top_features_rf['importance'], color='skyblue') # Alternative using Matplotlib\n",
        "\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.title(f'Top {top_n} Feature Importances (Random Forest - Tuned)')\n",
        "        plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "        plt.show() # Display the plot\n",
        "\n",
        "\n",
        "    # Handle potential errors during importance calculation or visualization\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 특성 중요도 확인 또는 시각화 중 오류 발생 (Error during feature importance check or visualization): {e}\")\n",
        "\n",
        "# Indicate the end of the notebook execution\n",
        "print(\"\\n--- 모든 작업 완료 (All tasks completed) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cc7d390"
      },
      "source": [
        "Now that we have the best parameters from the Randomized Search, we can train a final model with these parameters and evaluate its performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3da4acd7"
      },
      "outputs": [],
      "source": [
        "# 모델 평가 (Model Evaluation)\n",
        "if best_params_rf is not None and X_test is not None and y_test is not None:\n",
        "    print(\"\\n--- 모델 평가 시작 (Starting Model Evaluation) ---\")\n",
        "    try:\n",
        "        # 최적 파라미터로 최종 모델 학습 (Train final model with best parameters)\n",
        "        final_rf_model = RandomForestRegressor(**best_params_rf, random_state=42, n_jobs=-1)\n",
        "        final_rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # 테스트 데이터 예측 (Predict on test data)\n",
        "        y_pred = final_rf_model.predict(X_test)\n",
        "\n",
        "        # 성능 지표 계산 (Calculate performance metrics)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse) # RMSE\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        print(\"\\n✅ 모델 평가 완료 (Model evaluation complete)!\")\n",
        "        print(f\"✨ Mean Squared Error (MSE): {mse:.4f}\")\n",
        "        print(f\"✨ Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "        print(f\"✨ Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "        print(f\"✨ R-squared (R2): {r2:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 모델 평가 중 오류 발생 (Error during model evaluation): {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}